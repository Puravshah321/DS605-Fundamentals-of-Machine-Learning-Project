{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccb7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Total positives: 5177, total negatives before downsample: 5073168\n",
      "Final combined df shape: (1825177, 11)\n",
      "Preprocessing data...\n",
      "Numeric columns (after feature engineering): 25\n",
      "Categorical column cardinalities:\n",
      "  Account: 316281 unique values\n",
      "  Account.1: 311342 unique values\n",
      "  Receiving Currency: 15 unique values\n",
      "  Payment Currency: 15 unique values\n",
      "  Payment Format: 7 unique values\n",
      "Low-cardinality one-hot columns: ['Receiving Currency', 'Payment Currency', 'Payment Format']\n",
      "High-cardinality frequency-encoded columns: ['Account', 'Account.1']\n",
      "Preprocessing done. Shape: (1825177, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 15:14:04,768] A new study created in memory with name: no-name-1b0c849a-7bb1-4123-b14d-7ce89cf853f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1460141, 64) Val shape: (365036, 64)\n",
      "Starting trial: 0\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 15:15:48,244] Trial 0 finished with value: 0.44198855037817597 and parameters: {'learning_rate': 0.105112855976856, 'num_leaves': 87, 'min_data_in_leaf': 323, 'feature_fraction': 0.839798003361497, 'bagging_fraction': 0.683641576013517, 'bagging_freq': 6, 'lambda_l1': 3.02012210636246, 'lambda_l2': 3.327660787168076}. Best is trial 0 with value: 0.44198855037817597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 finished. Mean PR-AUC: 0.4420\n",
      "Starting trial: 1\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 15:17:40,254] Trial 1 finished with value: 0.43978373632746903 and parameters: {'learning_rate': 0.10257090153840911, 'num_leaves': 108, 'min_data_in_leaf': 218, 'feature_fraction': 0.868612046567186, 'bagging_fraction': 0.650230882068405, 'bagging_freq': 10, 'lambda_l1': 0.2979241522486714, 'lambda_l2': 3.3263826010925532}. Best is trial 0 with value: 0.44198855037817597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished. Mean PR-AUC: 0.4398\n",
      "Starting trial: 2\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 15:54:32,651] Trial 2 finished with value: 0.460415580658329 and parameters: {'learning_rate': 0.06719218114359057, 'num_leaves': 189, 'min_data_in_leaf': 499, 'feature_fraction': 0.7275012352524779, 'bagging_fraction': 0.7589182209524341, 'bagging_freq': 1, 'lambda_l1': 2.053564656276536, 'lambda_l2': 1.8220184672216204}. Best is trial 2 with value: 0.460415580658329.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 finished. Mean PR-AUC: 0.4604\n",
      "Starting trial: 3\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 15:55:47,159] Trial 3 finished with value: 0.4441917074385171 and parameters: {'learning_rate': 0.11751449658408694, 'num_leaves': 134, 'min_data_in_leaf': 298, 'feature_fraction': 0.6241473515730315, 'bagging_fraction': 0.9471979773725262, 'bagging_freq': 5, 'lambda_l1': 3.2864826398242593, 'lambda_l2': 4.111883517814671}. Best is trial 2 with value: 0.460415580658329.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 finished. Mean PR-AUC: 0.4442\n",
      "Starting trial: 4\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 15:59:00,575] Trial 4 finished with value: 0.4501726436966337 and parameters: {'learning_rate': 0.022838254135343253, 'num_leaves': 89, 'min_data_in_leaf': 158, 'feature_fraction': 0.6144141443860571, 'bagging_fraction': 0.9451867062601269, 'bagging_freq': 0, 'lambda_l1': 3.10984487525229, 'lambda_l2': 3.1395127225223445}. Best is trial 2 with value: 0.460415580658329.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 finished. Mean PR-AUC: 0.4502\n",
      "Starting trial: 5\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 18:00:43,410] Trial 5 finished with value: 0.41772182261576196 and parameters: {'learning_rate': 0.07010912075451153, 'num_leaves': 55, 'min_data_in_leaf': 185, 'feature_fraction': 0.8950248804351115, 'bagging_fraction': 0.7811810741523095, 'bagging_freq': 0, 'lambda_l1': 0.5241957588226287, 'lambda_l2': 0.7506942832909097}. Best is trial 2 with value: 0.460415580658329.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 finished. Mean PR-AUC: 0.4177\n",
      "Starting trial: 6\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 18:03:53,058] Trial 6 finished with value: 0.4705191517438958 and parameters: {'learning_rate': 0.02831801431478083, 'num_leaves': 150, 'min_data_in_leaf': 155, 'feature_fraction': 0.6478254128653478, 'bagging_fraction': 0.8488020322617558, 'bagging_freq': 7, 'lambda_l1': 4.7118528368351, 'lambda_l2': 4.0054122968063615}. Best is trial 6 with value: 0.4705191517438958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 finished. Mean PR-AUC: 0.4705\n",
      "Starting trial: 7\n",
      "  Fold 1/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1455998, 64) Positives: 727999 Negatives: 727999\n",
      "  Fold 2/2\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (1456000, 64) Positives: 728000 Negatives: 728000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 18:06:14,967] Trial 7 finished with value: 0.4332010817693698 and parameters: {'learning_rate': 0.03401780934686374, 'num_leaves': 114, 'min_data_in_leaf': 333, 'feature_fraction': 0.9665469599063847, 'bagging_fraction': 0.9646103064363295, 'bagging_freq': 10, 'lambda_l1': 0.3786059818679294, 'lambda_l2': 2.619132804105346}. Best is trial 6 with value: 0.4705191517438958.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 finished. Mean PR-AUC: 0.4332\n",
      "Best trial:\n",
      "  Value: 0.4705191517438958\n",
      "  Params:\n",
      "    learning_rate: 0.02831801431478083\n",
      "    num_leaves: 150\n",
      "    min_data_in_leaf: 155\n",
      "    feature_fraction: 0.6478254128653478\n",
      "    bagging_fraction: 0.8488020322617558\n",
      "    bagging_freq: 7\n",
      "    lambda_l1: 4.7118528368351\n",
      "    lambda_l2: 4.0054122968063615\n",
      "Best params and study saved.\n",
      "Balancing training data with RandomOverSampler...\n",
      "After resampling: (2911998, 64) Positives: 1455999 Negatives: 1455999\n",
      "Starting final LightGBM training...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.978139\tvalid_0's average_precision: 0.352922\n",
      "[200]\tvalid_0's auc: 0.979849\tvalid_0's average_precision: 0.428773\n",
      "[300]\tvalid_0's auc: 0.980032\tvalid_0's average_precision: 0.456152\n",
      "[400]\tvalid_0's auc: 0.980391\tvalid_0's average_precision: 0.476059\n",
      "[500]\tvalid_0's auc: 0.980524\tvalid_0's average_precision: 0.48865\n",
      "Early stopping, best iteration is:\n",
      "[505]\tvalid_0's auc: 0.980555\tvalid_0's average_precision: 0.48907\n",
      "Training done in 136.2 seconds.\n",
      "Model saved to ./output\\final_lgb_model.txt\n",
      "Final params saved.\n",
      "Validation predictions saved.\n",
      "Validation ROC-AUC: 0.9806, PR-AUC: 0.4888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    364001\n",
      "           1       0.13      0.74      0.21      1035\n",
      "\n",
      "    accuracy                           0.98    365036\n",
      "   macro avg       0.56      0.86      0.60    365036\n",
      "weighted avg       1.00      0.98      0.99    365036\n",
      "\n",
      "Confusion matrix:\n",
      " [[358676   5325]\n",
      " [   269    766]]\n"
     ]
    }
   ],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# !pip install -q pandas numpy scikit-learn lightgbm optuna joblib imbalanced-learn\n",
    "\n",
    "import os, time, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import lightgbm as lgb\n",
    "import optuna, joblib\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ===========================\n",
    "# USER CONFIG\n",
    "# ===========================\n",
    "\n",
    "DATA_PATH = r\"C:/Users/mannan/Desktop/ML_final/HI-Small_Trans.csv\"\n",
    "\n",
    "ID_COL = None\n",
    "TARGET_COL = \"Is Laundering\"\n",
    "\n",
    "CATEGORICAL_COLS = [\n",
    "    \"Account\",\n",
    "    \"Account.1\",\n",
    "    \"Receiving Currency\",\n",
    "    \"Payment Currency\",\n",
    "    \"Payment Format\"\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"From Bank\",\n",
    "    \"To Bank\",\n",
    "    \"Amount Received\",\n",
    "    \"Amount Paid\"\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"./output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 200_000\n",
    "KEEP_NEGATIVES = 70_000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "N_TRIALS = 8\n",
    "N_FOLDS = 2\n",
    "N_JOBS = 1\n",
    "\n",
    "BASE_LGB_PARAMS = {\n",
    "    'objective': 'binary',\n",
    "    'metric': ['auc', 'average_precision'],\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'max_bin': 255,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbosity': -1,\n",
    "    'num_threads': max(1, (os.cpu_count() or 2) - 1)\n",
    "}\n",
    "\n",
    "MAX_ONEHOT_CARDINALITY = 100\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 0. FEATURE ENGINEERING (UPDATED)\n",
    "# ============================================\n",
    "def add_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Add AML-style behavioral and interaction features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---------- Amount interaction features ----------\n",
    "    if {\"Amount Received\", \"Amount Paid\"}.issubset(df.columns):\n",
    "        df[\"amt_diff\"] = df[\"Amount Received\"] - df[\"Amount Paid\"]\n",
    "        df[\"amt_sum\"] = df[\"Amount Received\"] + df[\"Amount Paid\"]\n",
    "\n",
    "        denom = df[\"Amount Paid\"].replace(0, np.nan)\n",
    "        df[\"amt_ratio\"] = df[\"Amount Received\"] / denom\n",
    "        df[\"amt_ratio\"] = df[\"amt_ratio\"].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        df[\"log_amt_received\"] = np.log1p(df[\"Amount Received\"].clip(lower=0))\n",
    "        df[\"log_amt_paid\"] = np.log1p(df[\"Amount Paid\"].clip(lower=0))\n",
    "\n",
    "    # ---------- Relationship flags ----------\n",
    "    # Same currency? (cast to string to avoid categorical comparison error)\n",
    "    if {\"Receiving Currency\", \"Payment Currency\"}.issubset(df.columns):\n",
    "        rc = df[\"Receiving Currency\"].astype(str)\n",
    "        pc = df[\"Payment Currency\"].astype(str)\n",
    "        df[\"same_currency\"] = (rc == pc).astype(np.int8)\n",
    "\n",
    "    # Same bank?\n",
    "    if {\"From Bank\", \"To Bank\"}.issubset(df.columns):\n",
    "        df[\"same_bank\"] = (df[\"From Bank\"] == df[\"To Bank\"]).astype(np.int8)\n",
    "\n",
    "    # Same account? (cast to string to avoid categorical comparison error)\n",
    "    if {\"Account\", \"Account.1\"}.issubset(df.columns):\n",
    "        acc1 = df[\"Account\"].astype(str)\n",
    "        acc2 = df[\"Account.1\"].astype(str)\n",
    "        df[\"same_account\"] = (acc1 == acc2).astype(np.int8)\n",
    "\n",
    "    # ---------- Global amount z-scores ----------\n",
    "    for col in [\"Amount Received\", \"Amount Paid\"]:\n",
    "        if col in df.columns:\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            if std > 0:\n",
    "                df[f\"{col}_z\"] = (df[col] - mean) / std\n",
    "            else:\n",
    "                df[f\"{col}_z\"] = 0.0\n",
    "\n",
    "    # ---------- Account-level behavioral stats ----------\n",
    "    if \"Account\" in df.columns and \"Amount Paid\" in df.columns:\n",
    "        grp_sender = df.groupby(\"Account\")[\"Amount Paid\"]\n",
    "        df[\"acc_tx_count\"] = grp_sender.transform(\"count\").astype(np.int32)\n",
    "        df[\"acc_amt_paid_sum\"] = grp_sender.transform(\"sum\")\n",
    "        df[\"acc_amt_paid_mean\"] = grp_sender.transform(\"mean\")\n",
    "        df[\"acc_amt_paid_std\"] = grp_sender.transform(\"std\").fillna(0)\n",
    "\n",
    "    if \"Account.1\" in df.columns and \"Amount Received\" in df.columns:\n",
    "        grp_rcv = df.groupby(\"Account.1\")[\"Amount Received\"]\n",
    "        df[\"rcv_tx_count\"] = grp_rcv.transform(\"count\").astype(np.int32)\n",
    "        df[\"rcv_amt_received_sum\"] = grp_rcv.transform(\"sum\")\n",
    "        df[\"rcv_amt_received_mean\"] = grp_rcv.transform(\"mean\")\n",
    "        df[\"rcv_amt_received_std\"] = grp_rcv.transform(\"std\").fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 1. READ + DOWNSAMPLE\n",
    "# ============================================\n",
    "def read_and_downsample(\n",
    "    data_path,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    keep_negatives=KEEP_NEGATIVES,\n",
    "    target_col=TARGET_COL,\n",
    "    random_state=RANDOM_STATE\n",
    "):\n",
    "    dfs = []\n",
    "    pos_total, neg_total = 0, 0\n",
    "\n",
    "    for chunk_idx, chunk in enumerate(pd.read_csv(data_path, chunksize=chunk_size)):\n",
    "        print(f\"Processing chunk {chunk_idx+1}...\")\n",
    "\n",
    "        if target_col not in chunk.columns:\n",
    "            raise KeyError(\n",
    "                f\"Target column '{target_col}' not found in CSV. \"\n",
    "                f\"Available columns: {list(chunk.columns)}\"\n",
    "            )\n",
    "\n",
    "        chunk = chunk.dropna(subset=[target_col])\n",
    "\n",
    "        pos = chunk[chunk[target_col] == 1]\n",
    "        neg = chunk[chunk[target_col] == 0]\n",
    "\n",
    "        pos_total += len(pos)\n",
    "        neg_total += len(neg)\n",
    "\n",
    "        if len(neg) > keep_negatives:\n",
    "            neg = neg.sample(keep_negatives, random_state=random_state)\n",
    "\n",
    "        dfs.append(pd.concat([pos, neg], axis=0))\n",
    "\n",
    "    df = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "    print(f\"Total positives: {pos_total}, total negatives before downsample: {neg_total}\")\n",
    "    print(f\"Final combined df shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. PREPROCESS (USES FEATURE ENGINEERING)\n",
    "# ============================================\n",
    "def preprocess(\n",
    "    df,\n",
    "    id_col=ID_COL,\n",
    "    target_col=TARGET_COL,\n",
    "    cat_cols=CATEGORICAL_COLS,\n",
    "    num_cols=NUMERIC_COLS\n",
    "):\n",
    "    print(\"Preprocessing data...\")\n",
    "\n",
    "    if id_col is not None and id_col in df.columns:\n",
    "        df = df.drop(columns=[id_col])\n",
    "\n",
    "    if \"Timestamp\" in df.columns:\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "        df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
    "        df[\"Day\"] = df[\"Timestamp\"].dt.day\n",
    "        df[\"Weekday\"] = df[\"Timestamp\"].dt.weekday\n",
    "        df = df.drop(columns=[\"Timestamp\"])\n",
    "\n",
    "    if target_col in df.columns:\n",
    "        df[target_col] = df[target_col].astype(int)\n",
    "\n",
    "    cat_cols_in_df = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "    for col in cat_cols_in_df:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(\"category\")\n",
    "            .cat.add_categories(\"Missing\")\n",
    "            .fillna(\"Missing\")\n",
    "        )\n",
    "\n",
    "    # Add engineered features\n",
    "    df = add_feature_engineering(df)\n",
    "\n",
    "    # Numeric filling (all numeric except target)\n",
    "    numeric_cols_all = [\n",
    "        c for c in df.columns\n",
    "       if (is_numeric_dtype(df[c]) and c != target_col)\n",
    "    ]\n",
    "\n",
    "    print(\"Numeric columns (after feature engineering):\", len(numeric_cols_all))\n",
    "    for col in numeric_cols_all:\n",
    "        df[col] = df[col].astype(float)\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    cat_cols_in_df = [c for c in cat_cols_in_df if c in df.columns]\n",
    "\n",
    "    low_card_cols, high_card_cols = [], []\n",
    "\n",
    "    print(\"Categorical column cardinalities:\")\n",
    "    for col in cat_cols_in_df:\n",
    "        nunique = df[col].nunique()\n",
    "        print(f\"  {col}: {nunique} unique values\")\n",
    "        if nunique <= MAX_ONEHOT_CARDINALITY:\n",
    "            low_card_cols.append(col)\n",
    "        else:\n",
    "            high_card_cols.append(col)\n",
    "\n",
    "    # Frequency-encode high-cardinality categoricals\n",
    "    for col in high_card_cols:\n",
    "        freq = df[col].value_counts()\n",
    "        new_col = f\"{col}_freq\"\n",
    "        df[new_col] = df[col].map(freq)\n",
    "        df[new_col] = df[new_col].fillna(0)\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    print(f\"Low-cardinality one-hot columns: {low_card_cols}\")\n",
    "    print(f\"High-cardinality frequency-encoded columns: {high_card_cols}\")\n",
    "\n",
    "    df_encoded = pd.get_dummies(df, columns=low_card_cols, drop_first=True)\n",
    "\n",
    "    cols = [c for c in df_encoded.columns if c != target_col] + [target_col]\n",
    "    df_encoded = df_encoded[cols]\n",
    "\n",
    "    print(\"Preprocessing done. Shape:\", df_encoded.shape)\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. TRAIN / VAL SPLIT\n",
    "# ============================================\n",
    "def split_train_val(\n",
    "    df,\n",
    "    target_col=TARGET_COL,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    "):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(\"Train shape:\", X_train.shape, \"Val shape:\", X_val.shape)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. BALANCING & DATASET WRAPPER\n",
    "# ============================================\n",
    "def balance_training_data(X, y):\n",
    "    print(\"Balancing training data with RandomOverSampler...\")\n",
    "    ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    print(\n",
    "        \"After resampling:\",\n",
    "        X_res.shape,\n",
    "        \"Positives:\", int(y_res.sum()),\n",
    "        \"Negatives:\", int(len(y_res) - y_res.sum())\n",
    "    )\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def make_lgb_dataset(X, y):\n",
    "    return lgb.Dataset(X, label=y, free_raw_data=False)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. OPTUNA OBJECTIVE\n",
    "# ============================================\n",
    "def objective(trial, X, y, base_params):\n",
    "    print(f\"Starting trial: {trial.number}\")\n",
    "\n",
    "    params = base_params.copy()\n",
    "    params.update({\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 255),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 500),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
    "    })\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    pr_aucs, roc_aucs = [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"  Fold {fold+1}/{N_FOLDS}\")\n",
    "        X_tr, X_v = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_v = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        X_tr_bal, y_tr_bal = balance_training_data(X_tr, y_tr)\n",
    "        dtrain = make_lgb_dataset(X_tr_bal, y_tr_bal)\n",
    "        dval = lgb.Dataset(X_v, label=y_v, free_raw_data=False)\n",
    "\n",
    "        gbm = lgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=2000,\n",
    "            valid_sets=[dval],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50, verbose=False)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        y_pred = gbm.predict(X_v, num_iteration=gbm.best_iteration)\n",
    "        roc = roc_auc_score(y_v, y_pred)\n",
    "        p, r, _ = precision_recall_curve(y_v, y_pred)\n",
    "        pr = auc(r, p)\n",
    "\n",
    "        roc_aucs.append(roc)\n",
    "        pr_aucs.append(pr)\n",
    "\n",
    "    mean_pr = float(np.mean(pr_aucs))\n",
    "    print(f\"Trial {trial.number} finished. Mean PR-AUC: {mean_pr:.4f}\")\n",
    "    return mean_pr\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. LOAD + PREPROCESS + SPLIT\n",
    "# ============================================\n",
    "df_raw = read_and_downsample(DATA_PATH)\n",
    "df_processed = preprocess(df_raw)\n",
    "X_train_full, X_val, y_train_full, y_val = split_train_val(df_processed)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. HYPERPARAMETER TUNING\n",
    "# ============================================\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, X_train_full, y_train_full, BASE_LGB_PARAMS),\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"  Value:\", study.best_trial.value)\n",
    "print(\"  Params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 8. SAVE BEST PARAMS & STUDY\n",
    "# ============================================\n",
    "best_params = BASE_LGB_PARAMS.copy()\n",
    "best_params.update(study.best_trial.params)\n",
    "\n",
    "joblib.dump(best_params, os.path.join(OUTPUT_DIR, \"best_lgb_params.pkl\"))\n",
    "joblib.dump(study, os.path.join(OUTPUT_DIR, \"optuna_study.pkl\"))\n",
    "\n",
    "print(\"Best params and study saved.\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 9. FINAL TRAINING\n",
    "# ============================================\n",
    "final_params = best_params.copy()\n",
    "final_params['num_threads'] = max(1, (os.cpu_count() or 2) - 1)\n",
    "\n",
    "X_train_bal, y_train_bal = balance_training_data(X_train_full, y_train_full)\n",
    "dtrain = make_lgb_dataset(X_train_bal, y_train_bal)\n",
    "\n",
    "X_val = X_val[[c for c in X_train_full.columns if c != TARGET_COL]]\n",
    "y_val_arr = y_val.values\n",
    "dval = lgb.Dataset(X_val, label=y_val_arr)\n",
    "\n",
    "print(\"Starting final LightGBM training...\")\n",
    "t0 = time.time()\n",
    "gbm_full = lgb.train(\n",
    "    final_params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[dval],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"Training done in {(t1 - t0):.1f} seconds.\")\n",
    "\n",
    "model_path = os.path.join(OUTPUT_DIR, \"final_lgb_model.txt\")\n",
    "gbm_full.save_model(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "joblib.dump(final_params, os.path.join(OUTPUT_DIR, \"final_lgb_params.pkl\"))\n",
    "print(\"Final params saved.\")\n",
    "\n",
    "y_pred_val = gbm_full.predict(X_val, num_iteration=gbm_full.best_iteration)\n",
    "pd.DataFrame({'y_true': y_val_arr, 'y_prob': y_pred_val}).to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"val_predictions.csv\"),\n",
    "    index=False\n",
    ")\n",
    "print(\"Validation predictions saved.\")\n",
    "\n",
    "roc = roc_auc_score(y_val_arr, y_pred_val)\n",
    "p, r, _ = precision_recall_curve(y_val_arr, y_pred_val)\n",
    "prauc = auc(r, p)\n",
    "print(f\"Validation ROC-AUC: {roc:.4f}, PR-AUC: {prauc:.4f}\")\n",
    "print(classification_report(y_val_arr, (y_pred_val > 0.5).astype(int)))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_val_arr, (y_pred_val > 0.5).astype(int)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
